{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling (LDA)\n",
    "https://stackoverflow.com/questions/20349958/understanding-lda-implementation-using-gensim - tutorial\n",
    "https://stackoverflow.com/questions/19197757/remove-words-lines-between-matching-delimeters - regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/arisilburt/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS, remove_stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim\n",
    "import glob\n",
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 2830 distinct terms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "dir = 'readme_out'\n",
    "Documents = [x.strip() for x in open('%s/CORPUS.txt'%dir, 'r').readlines()]\n",
    "\n",
    "# stem words\n",
    "# sno = nltk.stem.SnowballStemmer('english')\n",
    "# for i in range(len(Documents)):\n",
    "#     text = Documents[i].split()\n",
    "#     for j in range(len(text)):\n",
    "#         text[j] = sno.stem(text[j])\n",
    "#     Documents[i] = ' '.join(e for e in text)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 10)\n",
    "A = vectorizer.fit_transform(Documents)\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'energy load time license open source python library analyze plot energy related kind data strong focus related energy electricity heat demand included carefully selected context generalized working different aim provide higher level commonly scientific analysis energy load statistical analysis convenience wrapper common statistical include analysis overview descriptive statistics reshape load duration curve extract daily plot plot generate generate daily monthly generate sinusoidal function sample given load duration curve given add noise noise correlated load fit analytical load duration statistics feature extraction quick overview load curve useful coupled machine learning library regression prediction numerous relevant around run following explore code import import load random rand create random vector convenience wrapper mean load duration nice useful dispatch daily notebook documentation construction overview available latest stable version install pip install aware library conceptual mode probably going change following want upgrade latest stable version use pip install ensure minimum satisfied current version want latest version git use development code git clone create automatically environment based environment source activate pip install install local ready run box distribution use think contribute new relevant improve code documentation way feel free contact repository send pull use library academic work consider python energy load time demand scientific image alt python target python license image alt target image target python image travis master target image badge alt target latest'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Documents[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=20, mean_change_tol=0.001,\n",
       "             n_components=3, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: data function use number value file type set example default output new object time input size array graph color return\n",
      "Topic #1: learning model data machine training deep neural network python train use test implementation loss image linear classification feature regression analysis\n",
      "Topic #2: install python use run data code project build file version available axe image need package source following create pip library\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words=20):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn \n",
    "https://github.com/derekgreene/topic-model-tutorial/blob/master/1%20-%20Text%20Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "EngWords = set(nltk.corpus.words.words())\n",
    "\n",
    "def clean_readme(readme):\n",
    "    preproc_text = open(readme, 'r', encoding='utf-8').read()\n",
    "    preproc_text.replace('\\n', '')\n",
    "    preproc_text = re.sub(r'<.*?>\\s*', '', preproc_text, flags=re.DOTALL)\n",
    "    preproc_text = re.sub(r'\\[.*?\\]\\s*', '', preproc_text, flags=re.DOTALL)\n",
    "    preproc_text = re.sub(r'\\(.*?\\)\\s*', '', preproc_text, flags=re.DOTALL)\n",
    "    text = re.sub('[^A-Za-z0-9 /.]+', '', preproc_text).lower()\n",
    "    cleaned_text = [token for token in simple_preprocess(remove_stopwords(text)) if len(token) > 2 and token in EngWords]\n",
    "    return \" \".join(cleaned_text)\n",
    "\n",
    "def create_corpus(dir, topics):\n",
    "    f = open('%sCORPUS.txt'%dir, 'w')\n",
    "    Corpus = []\n",
    "    for t in topics:\n",
    "        path = '%s%s'%(dir, t)\n",
    "        readmes = glob.glob('%s/*.txt'%path)\n",
    "        for r in readmes:\n",
    "            try:\n",
    "                Corpus.append(clean_readme(r))\n",
    "            except:\n",
    "                print('couldnt process %s'%r)\n",
    "\n",
    "    for doc in Corpus:\n",
    "        f.write(doc)\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    return Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'readme_out/'\n",
    "#topics = ['visualization', 'statistics', 'programming', 'machine-learning']\n",
    "topics = ['visualization', 'statistics', 'machine-learning']\n",
    "Documents = create_corpus(dir, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiple view interface proof concept prototype personal agency brushing linking giving people flexibility configure source link target multiple brushes information tutorial video supplemental getting try local dont install node install run install root directory project serve index file run application open browser way install install simple command line root directory project tell address open application browser tested chrome browser touch disable swipe browser history disable history navigation chrome'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Documents[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 2088 distinct terms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 20, max_df=0.3)\n",
    "#Documents = open('%s/CORPUS.txt'%dir, 'r').readlines()\n",
    "A = vectorizer.fit_transform(Documents)\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl') \n",
    "v = joblib.load('vectorizer.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def rank_terms( A, terms ):\n",
    "    # get the sums over each column\n",
    "    sums = A.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(terms):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. python (3563.00)\n",
      "02. learning (3271.00)\n",
      "03. model (3214.00)\n",
      "04. run (3055.00)\n",
      "05. file (2719.00)\n",
      "06. new (2242.00)\n",
      "07. number (2127.00)\n",
      "08. project (2092.00)\n",
      "09. set (1967.00)\n",
      "10. example (1948.00)\n",
      "11. machine (1930.00)\n",
      "12. build (1906.00)\n",
      "13. function (1906.00)\n",
      "14. image (1906.00)\n",
      "15. program (1791.00)\n",
      "16. version (1747.00)\n",
      "17. time (1726.00)\n",
      "18. write (1718.00)\n",
      "19. create (1715.00)\n",
      "20. need (1712.00)\n"
     ]
    }
   ],
   "source": [
    "ranking = rank_terms( A, terms )\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%02d. %s (%.2f)\" % ( i+1, pair[0], pair[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3522, 4)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "k = 4\n",
    "model = decomposition.NMF(init=\"nndsvd\", n_components=k ) \n",
    "# apply the model and extract the two factor matrices\n",
    "W = model.fit_transform(A).round(3)\n",
    "H = model.components_\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79, 2.58, 0.01, 1.79])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_index = terms.index('time')\n",
    "# round to 2 decimal places for display purposes\n",
    "H[:,term_index].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_descriptor( terms, H, topic_index, top ):\n",
    "    # reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # now get the terms corresponding to the top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 01: array, function, object, return, value, string, reduce, number, element, argument\n",
      "Topic 02: best, docker, list, git, star, web, free, security, build, source\n",
      "Topic 03: program, write, number, array, string, element, sum, decimal, integer, letter\n",
      "Topic 04: model, learning, training, set, machine, deep, neural, python, run, example\n"
     ]
    }
   ],
   "source": [
    "descriptors = []\n",
    "for topic_index in range(k):\n",
    "    descriptors.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
    "    str_descriptor = \", \".join( descriptors[topic_index] )\n",
    "    print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.58187758e-04, 1.49582631e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.18141303e-03, 6.36874055e-04, 0.00000000e+00],\n",
       "       [1.06355310e-03, 4.58930790e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.18141303e-03, 6.36874055e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [7.16696832e-04, 1.46538317e-04, 1.79728463e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.58187758e-04, 1.49582631e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.64365160e-04, 0.00000000e+00],\n",
       "       [1.55401805e-04, 3.05490558e-04, 0.00000000e+00],\n",
       "       [3.60335247e-03, 8.90080897e-04, 1.12211936e-04],\n",
       "       [1.15610671e-03, 1.98196358e-04, 0.00000000e+00],\n",
       "       [4.40025189e-04, 1.38665584e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.64365160e-04, 0.00000000e+00],\n",
       "       [1.55401805e-04, 3.05490558e-04, 0.00000000e+00],\n",
       "       [9.11489985e-04, 9.93601092e-05, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.13010894e-02, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.13010894e-02, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.13010894e-02, 0.00000000e+00],\n",
       "       [1.07412590e-03, 1.28052411e-03, 0.00000000e+00],\n",
       "       [7.73686098e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.95606923e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.11590381e-03, 4.59122936e-05, 0.00000000e+00],\n",
       "       [1.13980236e-04, 1.82661570e-04, 0.00000000e+00],\n",
       "       [2.39309804e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.06658530e-02, 1.33428013e-02, 2.11592767e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.11590381e-03, 4.59122936e-05, 0.00000000e+00],\n",
       "       [1.06658530e-02, 1.33428013e-02, 2.11592767e-03],\n",
       "       [1.06658530e-02, 1.33428013e-02, 2.11592767e-03],\n",
       "       [1.08905163e-03, 1.50193075e-04, 0.00000000e+00],\n",
       "       [4.14730382e-04, 6.74793890e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 6.13010894e-02, 0.00000000e+00],\n",
       "       [1.07412590e-03, 1.28052411e-03, 0.00000000e+00],\n",
       "       [1.06658530e-02, 1.33428013e-02, 2.11592767e-03],\n",
       "       [4.14730382e-04, 6.74793890e-04, 0.00000000e+00],\n",
       "       [1.07978299e-02, 4.74499188e-04, 0.00000000e+00],\n",
       "       [1.50364615e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.56484471e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.06658530e-02, 1.33428013e-02, 2.11592767e-03],\n",
       "       [2.12818824e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.56484471e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.29242361e-04, 2.06120289e-04, 0.00000000e+00],\n",
       "       [1.06658530e-02, 1.33428013e-02, 2.11592767e-03],\n",
       "       [2.03732890e-04, 0.00000000e+00, 1.53347801e-05],\n",
       "       [2.56484471e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.38078606e-04, 3.66744414e-04, 0.00000000e+00],\n",
       "       [6.45262479e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.06658530e-02, 1.33428013e-02, 2.11592767e-03],\n",
       "       [4.15198348e-03, 2.38391677e-04, 0.00000000e+00],\n",
       "       [2.56484471e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.14566807e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.07978299e-02, 4.74499188e-04, 0.00000000e+00],\n",
       "       [2.56484471e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.50606543e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.06060240e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.87995040e-04, 1.59683851e-06, 3.12313480e-05],\n",
       "       [4.42336491e-04, 4.04714703e-05, 0.00000000e+00],\n",
       "       [2.95102932e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.87995040e-04, 1.59683851e-06, 3.12313480e-05],\n",
       "       [4.42336491e-04, 4.04714703e-05, 0.00000000e+00],\n",
       "       [2.95102932e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.63285596e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.55201508e-03, 0.00000000e+00, 6.77616972e-04],\n",
       "       [3.51327099e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.07454146e-03, 2.07895082e-04, 0.00000000e+00],\n",
       "       [3.48823804e-03, 5.32658845e-04, 0.00000000e+00],\n",
       "       [1.39430061e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [7.36343972e-04, 2.61763403e-04, 0.00000000e+00],\n",
       "       [7.72318336e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.55201508e-03, 0.00000000e+00, 6.77616972e-04],\n",
       "       [1.07454146e-03, 2.07895082e-04, 0.00000000e+00],\n",
       "       [1.11590381e-03, 4.59122936e-05, 0.00000000e+00],\n",
       "       [1.06658530e-02, 1.33428013e-02, 2.11592767e-03],\n",
       "       [1.37463999e-03, 1.99585554e-04, 6.80231098e-05],\n",
       "       [7.64555645e-04, 1.70521044e-04, 0.00000000e+00],\n",
       "       [3.53547651e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.25895213e-03, 2.74772461e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [7.36343972e-04, 2.61763403e-04, 0.00000000e+00],\n",
       "       [6.90999931e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.06355310e-03, 4.58930790e-04, 0.00000000e+00],\n",
       "       [2.56484471e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.61938777e-04, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.55201508e-03, 0.00000000e+00, 6.77616972e-04],\n",
       "       [1.06658530e-02, 1.33428013e-02, 2.11592767e-03]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(vectorizer.transform([token for token in simple_preprocess(text) if token not in STOPWORDS and token in EngWords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
